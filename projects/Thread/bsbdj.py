

import requests
from lxml import etree
from urllib import request
import os
import re
from queue import Queue
import threading

# å®æˆ˜-ç™¾æ€ä¸å¾—å§ä¹‹å¤šçº¿ç¨‹å¼‚æ­¥çˆ¬çˆ¬å–å›¾ç‰‡
# 1.è·å–æ¯ä¸€é¡µçš„url main()å‡½æ•°å®Œæˆ
# 2.ç”Ÿäº§è€…ï¼ˆè·å–æ¯ä¸€é¡µçš„å›¾ç‰‡urlï¼‰
# 3.æ¯ä¸ªè¡¨æƒ…çš„url
# 4.æ¶ˆè´¹è€…ï¼ˆä¸‹è½½å›¾ç‰‡ï¼‰


#æŠŠé˜Ÿåˆ—ä¼ è¿›æ¥ï¼Œéœ€è¦é‡å†™initå‡½æ•°
class Procuder(threading.Thread):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'
    }

    # æ„é€ å‡½æ•°
    def __init__(self, page_queue, img_queue, *args, **kwargs):
        # é‡å†™initå‡½æ•° éœ€è¦è°ƒç”¨çˆ¶ç±»çš„initå‡½æ•°
        super(Procuder, self).__init__(*args, **kwargs)
        self.page_queue = page_queue
        self.img_queue = img_queue

    def run(self):
        while True:
            if self.page_queue.empty():
                break
            url, x = self.page_queue.get()
            self.parse_page(url, x)

    def parse_page(self, url, x):
        response = requests.get(url=url, headers=self.headers)
        text = response.text
        html = etree.HTML(text)
        imgs = html.xpath("//div[@class='j-r-list-c']//img")
        for img in imgs:
            img_urls = img.get('data-original')

            # è·å–å›¾ç‰‡çš„åå­—å’Œè·å–å›¾ç‰‡çš„åç¼€æ‰èƒ½æ‹¼æ¥ä¸€ä¸ªå›¾ç‰‡
            alts = img.get('alt')
            alt = re.sub(r'[\?ï¼Ÿ!ï¼ï¼Œ\\\nï½,\.ã€‚ã€ ï¼ˆï¼‰/\*\%ã€Šã€‹\ğŸ˜‚\ğŸ˜‚\ğŸ˜„\ğŸ˜Šâ€œâ€ï¼šã€ã€‘#~â€¦Â·Ô…(Â¯ï¹ƒ\Â¯\Ô…)|]', '', alts)[0:100]
            # print(str(len(alt))+'==='+alt)
            # osæ¨¡å—å¯ä»¥åˆ†å‰² . åé¢çš„åç¼€
            suffixs = os.path.splitext(img_urls)[1]

            if len(suffixs) == 0:
                suffix = re.sub('', '.jpg', suffixs)
                filename = alt + suffix
                self.img_queue.put([img_urls, filename, x])
            else:
                filename = alt + suffixs
                self.img_queue.put([img_urls, filename, x])


                # ä¿å­˜å›¾ç‰‡ä¸‹æ¥ï¼Œå¯ä»¥éå¸¸æ–¹ä¾¿ä¸‹è½½æ–‡ä»¶æˆ–è€…å›¾ç‰‡
                # request.urlretrieve(img_url, 'images/' + filename)
                # self.img_queue.put([img_urls, filename])


class Consumer(threading.Thread):
    # æ„é€ å‡½æ•°
    def __init__(self, page_queue, img_queue, *args, **kwargs):
        # é‡å†™initå‡½æ•° éœ€è¦è°ƒç”¨çˆ¶ç±»çš„initå‡½æ•°
        super(Consumer, self).__init__(*args, **kwargs)
        self.page_queue = page_queue
        self.img_queue = img_queue

    def run(self):
        while True:
            # å¦‚æœæ²¡æœ‰æ•°æ®
            if self.page_queue.empty() and self.img_queue.empty():
                break
            # è§£åŒ…

            img_url, filename, x = self.img_queue.get()

            request.urlretrieve(img_url, 'images/' + str(x) + '/' + filename)
            print(filename+'===ä¸‹è½½å®Œæˆ')


# åˆ›å»ºæ–‡ä»¶å¤¹   mkdir("E:\\Python\\demo01\\Thread\\bb")
def mkdir(path):
    # å»é™¤é¦–ä½ç©ºæ ¼
    path = path.strip()
    # å»é™¤å°¾éƒ¨ \ ç¬¦å·
    path = path.rstrip("\\")

    # åˆ¤æ–­è·¯å¾„æ˜¯å¦å­˜åœ¨
    # å­˜åœ¨     True
    # ä¸å­˜åœ¨   False
    isExists = os.path.exists(path)

    # åˆ¤æ–­ç»“æœ
    if not isExists:
        # å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç›®å½•
        # åˆ›å»ºç›®å½•æ“ä½œå‡½æ•°
        os.makedirs(path)
        print(path + ' åˆ›å»ºæˆåŠŸ')
        return True
    else:
        # å¦‚æœç›®å½•å­˜åœ¨åˆ™ä¸åˆ›å»ºï¼Œå¹¶æç¤ºç›®å½•å·²å­˜åœ¨
        print(path + 'ç›®å½•å·²å­˜åœ¨')
        return False


def main():
    page_queue = Queue(100)
    img_queue = Queue(1000)

    for x in range(1, 51):
        url = 'http://www.budejie.com/pic/%d' % x
        mkdir("E:\\Python\\demo01\\Thread\\images\\%d" % x)
        page_queue.put([url, x])

    for x in range(5):
        t = Procuder(page_queue, img_queue)
        t.start()

    for x in range(5):
        t = Consumer(page_queue, img_queue)
        t.start()


if __name__ == '__main__':
    main()
